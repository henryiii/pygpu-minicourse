{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Performance Python: GPUs\n",
    "## Henry Schreiner\n",
    "\n",
    "10-20-2021\n",
    "\n",
    "Survey: TBD\n",
    "\n",
    "Useful links:\n",
    "* [High Performance Python: CPUs](https://github.com/henryiii/python-performance-minicourse)\n",
    "* [Compiled Python](https://github.com/henryiii/python-compiled-minicourse)\n",
    "* [iscinumpy.gitlab.io](https://iscinumpy.gitlab.io)\n",
    "* [CompClass](https://github.com/henryiii/compclass)\n",
    "* [Level Up Your Python](https://henryiii.github.io/level-up-your-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to GPUs\n",
    "\n",
    "GPUs are \"graphics processing units\" designed to compute pixels on a screen. The massively parallel design can be useful for general purpose computing;\n",
    "GPU companies started providing ways to use GPUs as \"GPGPU\"s, general purpose GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing platform\n",
    "\n",
    "We will be using Conda, mostly through the conda-forge channel, which recently gained support for proper CUDA libraries. We are getting PyTorch from the torch channel, and Tensorflow 2.0 snuck in as well. Pip support for ML libraries is not too bad, either (both are rapidly improving).\n",
    "\n",
    "We will be using Python 3.8. Numba does not have a 3.9 compatible release yet, since the bytecode changed and that affected libraries that inspect bytecode, like Numba does. A 3.8 compatible release is due this month. PyTorch and TensorFlow also do not support Python 3.9 yet. So far, of the major libraries discussed, CuPy will likely be the first to Python 3.9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Languages/platforms\n",
    "\n",
    "For differences in terminology, the ROCm page is quite good: <https://rocm.github.io/languages.html>.\n",
    "\n",
    "![Language interest](images/LanguageInterest.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CUDA\n",
    "\n",
    "The leader in the pack is easily NVidia; they were first to the foray with the CUDA language, and they easily lead for scientific computation.\n",
    "\n",
    "\n",
    "* Wildly popular\n",
    "* NVidia only\n",
    "* A C++-like language, single source (with JIT option)\n",
    "\n",
    "#### OpenCL\n",
    "\n",
    "AMD was late to the game, and tried to support an open standard, OpenCL - but poor support from other players caused it to be almost AMD exclusive.\n",
    "Apple released Metal as a replacement for OpenGL & OpenCL; they have worked with Intel & AMD on it. The are dropping their (almost non-existent) support for OpenCL.\n",
    "The Kronos group (which works on OpenGL/CL) has released a successor, Vulkan, but it mostly focuses on graphics (OpenGL) at the moment.\n",
    "Intel is planning to drop OpenCL in 2-3 years, too.\n",
    "\n",
    "* Works on most platforms\n",
    "* Most platforms have buggy, older support execpt AMD\n",
    "* Only JIT-like option\n",
    "* Also supports other compute backends, like FPGAs and CPUs\n",
    "\n",
    "\n",
    "\n",
    "#### ROCm\n",
    "\n",
    "* AMD only\n",
    "* Open, interacts with others at various levels\n",
    "\n",
    "AMD's custom platform is ROCm, which is their custom platform.\n",
    "\n",
    "#### SYCL\n",
    "\n",
    "SYCL was a CUDA-like single source language built on OpenCL, and is now part of Intel's OneAPI plan.\n",
    "\n",
    "\n",
    "#### OpenMP\n",
    "\n",
    "OpenMP now has tools to target GPUs, but it can be tricky to program (especially if you expect to write the same code to run multiple places). There's also OpenACC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will focus on CUDA, since it has good Python support and is the current lingua franca for scientific computing. OpenCL is not as popular, but has some Python libraries. ROCm recently has been showing up in Numba and CuPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries\n",
    "\n",
    "In the CPU class, we covered several libraries, but Numba was a clear standout in terms of high performance and ease of use. In GPU computing, the landscape is still quite varied. It's much harder to select a clear winner; each has features and drawbacks.\n",
    "\n",
    "![Library interest](images/LibraryInterest.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CuPy\n",
    "\n",
    "This was designed for the ML framework Chainer, but it becoming quite popular on its own.\n",
    "\n",
    "* *Very* close (often drop in replacement) for NumPy\n",
    "* Custom kernel support, including element-wise and reduction kernels\n",
    "     * Written in CUDA\n",
    "* Fusion support (although limited)\n",
    "* Rapid development\n",
    "* Experimental ROCm support\n",
    "* Supports Numba's GPU array interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numba\n",
    "\n",
    "This comes up again, since it has a GPU mode too!\n",
    "\n",
    "* Powerful but limited vectorize (elementwise UFunct)\n",
    "* Full kernel mode, but hand launched\n",
    "    * Written in Python subset\n",
    "* Device function support\n",
    "* New ROCm mode, but different terms\n",
    "* Developed the GPU array interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "\n",
    "This is Facebook's ML library.\n",
    "\n",
    "* NumPy-like\n",
    "* Has tape-based gradient support\n",
    "* Has fusion mode (torch-script), can support multiple languages\n",
    "* Hard to make custom kernels\n",
    "* Supports Numba's GPU array interface\n",
    "* *Great* tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow\n",
    "\n",
    "This is Google's ML library.\n",
    "\n",
    "* New API is similar to PyTorch, but might be a bit slow currently\n",
    "* Fusion mode builds graph, but still slower than API 1\n",
    "* API 1 was very fast, but hard to *setup* (computations easy, though)\n",
    "* Hard to make custom kernels\n",
    "* Lucky to support NumPy's array interface; no GPU interface (yet?)\n",
    "* Multiple language backends, including Swift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU basics\n",
    "\n",
    "GPU programming has several characteristics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory\n",
    "\n",
    "GPU memory is separate from main memory, and the transfer cost is high. You will constantly be thinking about *where* you memory is, and how to reduce the transfer of that memory between host and device.\n",
    "\n",
    "Note that there are techniques, like pinned/universal memory that can hide this from the programmer to an extent.\n",
    "\n",
    "Also there are several types of memory, going from local to global, along with specialized memories like constant and texture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel computation\n",
    "\n",
    "GPU threads often are tempting to think of as CPU threads, but they are must more like vector registers. A GPU computes a \"warp\" at a time (32 threads); each thread has does the *same* computation. So, for example, how many times will the following code run:\n",
    "\n",
    "```python\n",
    "if x < 0:\n",
    "    x = 0\n",
    "else:\n",
    "    x = x\n",
    "```\n",
    "\n",
    "This will first run and compute x<0 and create a mask. It will then run `x = 0` with some threads masked, then `x = x` with the other threads masked!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronization\n",
    "\n",
    "GPUs can operate on streams (somewhat like threads in CPU programming). You can give one stream commands to work on while the other stream is loading data. However, this means a lot of commands are asynchronous, that is, they return immediately and just schedule work to be done, rather than waiting till after the work is done to return. If you are using the results, this is fine (things wait properly), but if you are timing runs, you should have a \"synchronize\" step to make sure work is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching (and other smart CPU things)\n",
    "\n",
    "GPUs are not as smart as CPUs, and cannot do as much branch prediction and caching as a CPU can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other parallel concepts, like atomics, still apply for GPUs as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple GPUs\n",
    "\n",
    "You may have multiple GPUs connected to a single CPU system. Most GPU libraries have a context system that lets you switch between the GPUs, but it's usually another thing you have to program for."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyGPU Course 2019/12 [course/pygpu/default]",
   "language": "python",
   "name": "sys_pygpu201912"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
