{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting: Computing an NLL\n",
    "\n",
    "We will be using PyTorch this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "dist = np.hstack(\n",
    "    [\n",
    "        np.random.normal(loc=1, scale=2.0, size=500_000),\n",
    "        np.random.normal(loc=1, scale=0.5, size=500_000),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch: CPU\n",
    "By default, Torch data will be on the CPU unless sent to a GPU. Let's start with CPU, then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_dist = torch.tensor(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is similar to NumPy, though we'll have to be careful to use a non-Torch `sqrt` function since it does not operate on a Torch Tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x, μ, σ):\n",
    "    return (\n",
    "        1 / torch.sqrt(2 * np.pi * σ**2) * torch.exp(-((x - μ) ** 2) / (2 * σ**2))\n",
    "    )\n",
    "\n",
    "\n",
    "def add(x, f_0, mean, sigma, sigma2):\n",
    "    return f_0 * gaussian(x, mean, sigma) + (1 - f_0) * gaussian(x, mean, sigma2)\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def nll(dist, f_0, mean, sigma, sigma2):\n",
    "    return -torch.sum(torch.log(add(dist, f_0, mean, sigma, sigma2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "vals = [torch.tensor(v) for v in np.random.rand(4)]\n",
    "nll(d_dist, *vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch: GPU\n",
    "\n",
    "Moving this to the GPU is very simple; we get a CUDA device and then use `.to` to send data to the device. *Note that we do not have to send functions to the device, only data. If you are doing ML, models usually also have to be sent to the device, because they contain weights, and weights are data)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dist = d_dist.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Warning: in the current environment, this is a little broken - PyTorch and conda-forge are conflicting, I believe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "vals = [torch.tensor(v).to(device) for v in np.random.rand(4)]\n",
    "nll(dev_dist, *vals)\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Try enabling the `torch.jit.script` decorator. What happens to the performance? How does it compare with the other methods now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch gradients\n",
    "\n",
    "Torch's strong point (along with TensorFlow) is the gradient functionality. If you make a tensor with `requires_grad=True`, it then keeps a record of what happens to it during calculations, called a tape. If you call `result.backward(values)`, it replays the tape of gradient operations in reverse, allowing you to get the gradient. This is very powerful in fitting problems, such as those encountered in ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyGPU Course 2019/12 [course/pygpu/default]",
   "language": "python",
   "name": "sys_pygpu201912"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
